Total train data points: 608, Won: 42.26973684210526%, Lost: 34.046052631578945%, Draw: 23.68421052631579%
Total val data points: 76, Won: 36.8421052631579%, Lost: 39.473684210526315%, Draw: 23.68421052631579%
Total test data points: 76, Won: 40.78947368421053%, Lost: 42.10526315789474%, Draw: 17.105263157894736%
GNN model, data {} 0
Continuous evaluation
T:0, train_loss:1.11177, train_acc:0.31852, val_loss=1.09857, val_acc=0.35340
T:1, train_loss:1.10020, train_acc:0.43016, val_loss=1.09626, val_acc=0.34259
T:2, train_loss:1.14936, train_acc:0.35435, val_loss=1.08690, val_acc=0.40432
T:3, train_loss:1.15031, train_acc:0.35965, val_loss=1.07120, val_acc=0.42747
T:4, train_loss:1.05388, train_acc:0.39415, val_loss=1.09569, val_acc=0.37191
T:5, train_loss:1.08082, train_acc:0.40598, val_loss=1.12802, val_acc=0.42723
T:6, train_loss:1.05015, train_acc:0.40049, val_loss=1.20079, val_acc=0.41429
T:7, train_loss:1.01293, train_acc:0.42812, val_loss=1.27965, val_acc=0.41707
T:8, train_loss:1.01225, train_acc:0.43611, val_loss=1.46699, val_acc=0.41830
T:9, train_loss:1.03116, train_acc:0.43667, val_loss=1.60185, val_acc=0.43615
T:10, train_loss:1.03353, train_acc:0.42745, val_loss=1.91960, val_acc=0.41582
T:11, train_loss:1.06796, train_acc:0.43970, val_loss=2.85918, val_acc=0.43419
T:12, train_loss:1.03795, train_acc:0.43694, val_loss=1.98120, val_acc=0.43576
T:13, train_loss:1.04994, train_acc:0.43612, val_loss=2.71695, val_acc=0.43915
T:14, train_loss:1.01417, train_acc:0.46811, val_loss=3.29636, val_acc=0.45161
T:15, train_loss:1.01698, train_acc:0.50926, val_loss=4.76697, val_acc=0.48087
T:16, train_loss:0.99963, train_acc:0.52023, val_loss=3.49258, val_acc=0.49630
T:17, train_loss:0.99362, train_acc:0.54012, val_loss=4.35727, val_acc=0.49906
T:18, train_loss:1.00830, train_acc:0.51782, val_loss=4.06043, val_acc=0.48467
T:19, train_loss:1.01440, train_acc:0.52143, val_loss=6.10997, val_acc=0.48343
T:20, train_loss:1.00555, train_acc:0.52217, val_loss=6.58103, val_acc=0.50000
T:21, train_loss:1.00109, train_acc:0.52032, val_loss=6.67096, val_acc=0.49899
T:22, train_loss:0.99819, train_acc:0.51567, val_loss=5.95624, val_acc=0.49588
T:23, train_loss:1.00371, train_acc:0.51012, val_loss=5.38128, val_acc=0.49686
T:24, train_loss:0.99879, train_acc:0.50160, val_loss=4.87407, val_acc=0.49786
T:25, train_loss:0.99863, train_acc:0.50765, val_loss=6.56517, val_acc=0.50980
T:26, train_loss:0.99514, train_acc:0.50818, val_loss=6.37221, val_acc=0.51111
T:27, train_loss:0.99949, train_acc:0.50535, val_loss=7.22894, val_acc=0.50340
T:28, train_loss:0.99354, train_acc:0.50993, val_loss=7.83684, val_acc=0.48843
T:29, train_loss:0.99400, train_acc:0.50887, val_loss=6.96971, val_acc=0.48936
T:30, train_loss:0.99871, train_acc:0.50438, val_loss=5.31412, val_acc=0.48792
T:31, train_loss:0.99501, train_acc:0.51090, val_loss=6.95209, val_acc=0.48395
T:32, train_loss:0.99465, train_acc:0.50865, val_loss=6.95644, val_acc=0.48990
T:33, train_loss:0.99442, train_acc:0.51924, val_loss=8.68172, val_acc=0.45995
T:34, train_loss:0.99177, train_acc:0.52835, val_loss=6.59406, val_acc=0.49206
T:35, train_loss:0.98918, train_acc:0.52883, val_loss=6.08943, val_acc=0.46883
T:36, train_loss:0.98079, train_acc:0.54013, val_loss=6.85565, val_acc=0.49167
T:37, train_loss:0.98940, train_acc:0.53137, val_loss=6.64818, val_acc=0.49003
T:38, train_loss:0.99727, train_acc:0.52111, val_loss=7.57191, val_acc=0.48538
T:39, train_loss:1.01039, train_acc:0.50953, val_loss=5.76714, val_acc=0.45045
T:40, train_loss:1.01394, train_acc:0.51244, val_loss=7.09365, val_acc=0.48148
T:41, train_loss:1.00886, train_acc:0.51318, val_loss=5.18616, val_acc=0.46984
T:42, train_loss:1.00265, train_acc:0.51509, val_loss=7.26368, val_acc=0.47712
T:43, train_loss:0.99749, train_acc:0.52747, val_loss=6.25319, val_acc=0.44781
T:44, train_loss:1.16357, train_acc:0.52944, val_loss=6.37639, val_acc=0.44792
T:45, train_loss:1.07047, train_acc:0.52223, val_loss=3.21374, val_acc=0.48746
T:46, train_loss:1.06528, train_acc:0.52427, val_loss=2.44495, val_acc=0.48148
T:47, train_loss:1.04160, train_acc:0.51977, val_loss=1.43980, val_acc=0.49042
T:48, train_loss:1.01995, train_acc:0.51805, val_loss=1.44933, val_acc=0.50397
T:49, train_loss:1.01743, train_acc:0.52111, val_loss=1.06751, val_acc=0.51852
T:50, train_loss:1.01808, train_acc:0.51816, val_loss=1.23641, val_acc=0.49145
T:51, train_loss:1.00513, train_acc:0.52237, val_loss=1.12139, val_acc=0.50222
T:52, train_loss:1.03510, train_acc:0.48660, val_loss=1.08051, val_acc=0.49537
T:53, train_loss:1.01798, train_acc:0.51232, val_loss=1.10629, val_acc=0.44444
T:54, train_loss:1.00632, train_acc:0.52285, val_loss=1.08472, val_acc=0.45455
T:55, train_loss:1.00323, train_acc:0.52852, val_loss=1.08011, val_acc=0.45503
T:56, train_loss:1.01132, train_acc:0.52255, val_loss=1.10644, val_acc=0.46667
T:57, train_loss:1.01673, train_acc:0.50967, val_loss=1.08735, val_acc=0.48538
T:58, train_loss:1.01537, train_acc:0.50322, val_loss=1.08402, val_acc=0.48148
T:59, train_loss:1.00035, train_acc:0.51701, val_loss=1.05210, val_acc=0.52288
T:60, train_loss:1.00886, train_acc:0.51201, val_loss=1.07779, val_acc=0.46528
T:61, train_loss:1.01587, train_acc:0.50828, val_loss=1.10181, val_acc=0.48889
T:62, train_loss:1.01189, train_acc:0.52376, val_loss=1.08684, val_acc=0.46032
T:63, train_loss:1.02206, train_acc:0.50515, val_loss=1.11571, val_acc=0.41880
T:64, train_loss:1.01655, train_acc:0.51051, val_loss=1.07555, val_acc=0.45370
T:65, train_loss:1.00814, train_acc:0.52707, val_loss=1.06359, val_acc=0.48485
T:66, train_loss:1.00485, train_acc:0.52575, val_loss=1.08688, val_acc=0.45556
T:67, train_loss:1.00407, train_acc:0.52767, val_loss=1.07939, val_acc=0.43210
T:68, train_loss:1.00907, train_acc:0.52129, val_loss=1.04982, val_acc=0.50000
T:69, train_loss:1.00673, train_acc:0.52478, val_loss=1.05046, val_acc=0.46032
T:70, train_loss:1.01015, train_acc:0.51737, val_loss=1.04072, val_acc=0.48148
T:71, train_loss:1.00545, train_acc:0.52255, val_loss=1.03196, val_acc=0.51111
T:72, train_loss:1.00315, train_acc:0.52020, val_loss=1.04090, val_acc=0.55556
T:73, train_loss:0.99732, train_acc:0.52692, val_loss=1.02110, val_acc=0.55556
T:74, train_loss:1.00188, train_acc:0.52321, val_loss=1.06900, val_acc=0.44444
T:75, train_loss:1.01912, train_acc:0.50678, val_loss=0.96930, val_acc=0.55556
0.4753704462896583
accuracy on testing data is: 0.5263157894736842